{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = './Dataset/ham/'\n",
    "SPAM_DIR = './Dataset/spam/'\n",
    "HAM_CLEAN_DIR = './Dataset/ham_clean/'\n",
    "SPAM_CLEAN_DIR = './Dataset/spam_clean/'\n",
    "FEATURES_FILE_DIR = './Dataset/features.txt'\n",
    "ENG_VOCAB_FEATURES_FILE_DIR = './Dataset/eng_vocab_features.txt'\n",
    "CLEANR_HTML = re.compile('(<.*?>|&[a-zA-Z]+;)')\n",
    "URL = re.compile(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\")\n",
    "EMAIL = re.compile('[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}')\n",
    "NUMBER = re.compile('[0-9]+')\n",
    "DOLLAR = re.compile(\"\\$\")\n",
    "SPACE = re.compile(\"( |\\t)+\")\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "eng_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files len :  6951\n"
     ]
    }
   ],
   "source": [
    "for _ ,__ ,files in os.walk(HAM_DIR):\n",
    "    print(\"files len : \",len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_till_empty(lines , i):\n",
    "    while i+1 < len(lines) and lines[i]!=\"\\n\":\n",
    "       i+=1\n",
    "    return i   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(filename):\n",
    "    with open(filename,encoding = \"ISO-8859-1\") as f:\n",
    "           lines = f.readlines()\n",
    "        \n",
    "    i = walk_till_empty(lines, 0)\n",
    "    while len(lines[i+1].strip().split(\" \")[0].lower()) == 0 or (lines[i+1].strip().split(\" \")[0].lower()[0] >= 'a' and lines[i+1].strip().split(\" \")[0].lower()[0] <= 'z' and (not re.match(URL,lines[i+1]) and lines[i+1].strip().split(\" \")[0].find(\":\") != -1)):\n",
    "        i = walk_till_empty(lines, i+1)\n",
    "\n",
    "    return lines[i+1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[line for line in extract_email('./Dataset/spam/00005.ed0aba4d386c5e62bc737cf3f0ed9589') if(not len(line.strip()) == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ham_clean():\n",
    "    for _ ,__ ,ham_files in os.walk(HAM_DIR):\n",
    "        print(\"ham files found : \",len(files))\n",
    "    n = 0\n",
    "    for ham_file in ham_files :\n",
    "        outF = open(f\"{HAM_CLEAN_DIR}ham{n}.txt\", \"w\",encoding = \"ISO-8859-1\")\n",
    "        n+=1\n",
    "        glob = \"\"\n",
    "        for line in extract_email(f\"{HAM_DIR}{ham_file}\"):\n",
    "            if(not len(line.strip()) == 0):\n",
    "                glob+=\" \"+line\n",
    "        \n",
    "        line = re.sub(SPACE,\" \",glob.lower().replace(\"\\n\",\" \").strip())\n",
    "        \n",
    "        line = re.sub(CLEANR_HTML, '', line)\n",
    "        line = re.sub(URL,\"httpaddr\",line)\n",
    "        line = re.sub(EMAIL,\"emailaddr\",line)\n",
    "        line = re.sub(NUMBER,\"number\",line)\n",
    "        line = re.sub(DOLLAR,\"dollar\",line)\n",
    "        \n",
    "        # NLTK JOB\n",
    "        line = [lemmatizer.lemmatize(word) for word in word_tokenize(line) if word not in eng_stop_words]\n",
    "        \n",
    "        line = \" \".join(line)\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokenized = word_tokenize(line)\n",
    "        for word in tokenized :\n",
    "            if word not in words.keys() :\n",
    "                words[word] = 1\n",
    "            else :\n",
    "                words[word]+= 1\n",
    "\n",
    "        print(line,file=outF)\n",
    "\n",
    "\n",
    "            \n",
    "        outF.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spam_clean():\n",
    "    for _ ,__ ,spam_files in os.walk(SPAM_DIR):\n",
    "        print(\"spam files found : \",len(spam_files))\n",
    "    n = 0\n",
    "    for spam_file in spam_files :\n",
    "        outF = open(f\"{SPAM_CLEAN_DIR}spam{n}.txt\", \"w\",encoding = \"ISO-8859-1\")\n",
    "        n+=1\n",
    "        glob = \"\"\n",
    "        try :\n",
    "            mail = extract_email(f\"{SPAM_DIR}{spam_file}\")\n",
    "            for line in mail:\n",
    "                if(not len(line.strip()) == 0):\n",
    "                   glob+=\" \"+line   \n",
    "        except Exception:\n",
    "            print(spam_file)\n",
    "        \n",
    "        \n",
    "        line = re.sub(SPACE,\" \",glob.lower().replace(\"\\n\",\" \").strip())\n",
    "        \n",
    "        line = re.sub(CLEANR_HTML, '', line)\n",
    "        line = re.sub(URL,\"httpaddr\",line)\n",
    "        line = re.sub(EMAIL,\"emailaddr\",line)\n",
    "        line = re.sub(NUMBER,\"number\",line)\n",
    "        line = re.sub(DOLLAR,\"dollar\",line)\n",
    "        \n",
    "        # NLTK JOB\n",
    "        line = [lemmatizer.lemmatize(word) for word in word_tokenize(line) if word not in eng_stop_words]\n",
    "        line = \" \".join(line)\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokenized = word_tokenize(line)\n",
    "        for word in tokenized :\n",
    "            if word not in words.keys() :\n",
    "                words[word] = 1\n",
    "            else :\n",
    "                words[word]+= 1\n",
    "\n",
    "        print(line,file=outF)\n",
    "\n",
    "\n",
    "            \n",
    "        outF.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham files found :  6951\n"
     ]
    }
   ],
   "source": [
    "create_ham_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam files found :  2397\n"
     ]
    }
   ],
   "source": [
    "create_spam_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76928"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedwords = {k: v for k, v in sorted(words.items(), key=lambda item: item[1],reverse=True) if v > 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedwords.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ' '.join([key for key in sortedwords.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresFile = open(FEATURES_FILE_DIR, \"w\",encoding = \"ISO-8859-1\")\n",
    "print(features,file=featuresFile)\n",
    "featuresFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_english_vocab_features = {k: v for k, v in sorted(words.items(), key=lambda item: item[1],reverse=True) if v > 100 and k in english_vocab and len(k)>=2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2341"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_english_vocab_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_features = ' '.join([key for key in sorted_english_vocab_features.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_featuresFile = open(ENG_VOCAB_FEATURES_FILE_DIR, \"w\",encoding = \"ISO-8859-1\")\n",
    "print(eng_vocab_features,file=eng_vocab_featuresFile)\n",
    "eng_vocab_featuresFile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ca816f629268e8059debce0040472b46a057474b2ba3c5cb42c2b160ad6c3a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('tf2env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
